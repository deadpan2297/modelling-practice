{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2nPv29Lp1IE",
        "outputId": "cbe21301-4b34-4853-ec6d-05edf0a02a3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "#Necessary imports\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "import torch\n",
        "from scipy.integrate import odeint\n",
        "#from google.colab import output as colab_output\n",
        "\n",
        "# Setup device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-H5x7Rn-mwVb"
      },
      "source": [
        "We will try to learn the flow map for the 2D ode with 1 parameter\n",
        "$$\\begin{cases} x' &= \\mu - x^2 \\\\\n",
        "y' &= -y\n",
        "\\end{cases}.$$\n",
        "\n",
        "We want our training data to be of the form\n",
        "$$(x_j^{(1)}, y_j^{(1)}, x_j^{(2)}, y_j^{(2)}, \\mu_j)$$\n",
        "so first lets collect the data using scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gej_5zasml4a",
        "outputId": "8702e81c-92de-46d1-feca-93df81f640b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0% complete\n",
            "0.3% complete\n",
            "0.7% complete\n",
            "1.0% complete\n",
            "1.3% complete\n",
            "1.7% complete\n",
            "2.0% complete\n",
            "2.3% complete\n",
            "2.7% complete\n",
            "3.0% complete\n",
            "3.3% complete\n",
            "3.7% complete\n",
            "4.0% complete\n",
            "4.3% complete\n",
            "4.7% complete\n",
            "5.0% complete\n",
            "5.3% complete\n",
            "5.7% complete\n",
            "6.0% complete\n",
            "6.3% complete\n",
            "6.7% complete\n",
            "7.0% complete\n",
            "7.3% complete\n",
            "7.7% complete\n",
            "8.0% complete\n",
            "8.3% complete\n",
            "8.7% complete\n",
            "9.0% complete\n",
            "9.3% complete\n",
            "9.7% complete\n",
            "10.0% complete\n",
            "10.3% complete\n",
            "10.7% complete\n",
            "11.0% complete\n",
            "11.3% complete\n",
            "11.7% complete\n",
            "12.0% complete\n",
            "12.3% complete\n",
            "12.7% complete\n",
            "13.0% complete\n",
            "13.3% complete\n",
            "13.7% complete\n",
            "14.0% complete\n",
            "14.3% complete\n",
            "14.7% complete\n",
            "15.0% complete\n",
            "15.3% complete\n",
            "15.7% complete\n",
            "16.0% complete\n",
            "16.3% complete\n",
            "16.7% complete\n",
            "17.0% complete\n",
            "17.3% complete\n",
            "17.7% complete\n",
            "18.0% complete\n",
            "18.3% complete\n",
            "18.7% complete\n",
            "19.0% complete\n",
            "19.3% complete\n",
            "19.7% complete\n",
            "20.0% complete\n",
            "20.3% complete\n",
            "20.7% complete\n",
            "21.0% complete\n",
            "21.3% complete\n",
            "21.7% complete\n",
            "22.0% complete\n",
            "22.3% complete\n",
            "22.7% complete\n",
            "23.0% complete\n",
            "23.3% complete\n",
            "23.7% complete\n",
            "24.0% complete\n",
            "24.3% complete\n",
            "24.7% complete\n",
            "25.0% complete\n",
            "25.3% complete\n",
            "25.7% complete\n",
            "26.0% complete\n",
            "26.3% complete\n",
            "26.7% complete\n",
            "27.0% complete\n",
            "27.3% complete\n",
            "27.7% complete\n",
            "28.0% complete\n",
            "28.3% complete\n",
            "28.7% complete\n",
            "29.0% complete\n",
            "29.3% complete\n",
            "29.7% complete\n",
            "30.0% complete\n",
            "30.3% complete\n",
            "30.7% complete\n",
            "31.0% complete\n",
            "31.3% complete\n",
            "31.7% complete\n",
            "32.0% complete\n",
            "32.3% complete\n",
            "32.7% complete\n",
            "33.0% complete\n",
            "33.3% complete\n",
            "33.7% complete\n",
            "34.0% complete\n",
            "34.3% complete\n",
            "34.7% complete\n",
            "35.0% complete\n",
            "35.3% complete\n",
            "35.7% complete\n",
            "36.0% complete\n",
            "36.3% complete\n",
            "36.7% complete\n",
            "37.0% complete\n",
            "37.3% complete\n",
            "37.7% complete\n",
            "38.0% complete\n",
            "38.3% complete\n",
            "38.7% complete\n",
            "39.0% complete\n",
            "39.3% complete\n",
            "39.7% complete\n",
            "40.0% complete\n",
            "40.3% complete\n",
            "40.7% complete\n",
            "41.0% complete\n",
            "41.3% complete\n",
            "41.7% complete\n",
            "42.0% complete\n",
            "42.3% complete\n",
            "42.7% complete\n",
            "43.0% complete\n",
            "43.3% complete\n",
            "43.7% complete\n",
            "44.0% complete\n",
            "44.3% complete\n",
            "44.7% complete\n",
            "45.0% complete\n",
            "45.3% complete\n",
            "45.7% complete\n",
            "46.0% complete\n",
            "46.3% complete\n",
            "46.7% complete\n",
            "47.0% complete\n",
            "47.3% complete\n",
            "47.7% complete\n",
            "48.0% complete\n",
            "48.3% complete\n",
            "48.7% complete\n",
            "49.0% complete\n",
            "49.3% complete\n",
            "49.7% complete\n",
            "50.0% complete\n",
            "50.3% complete\n",
            "50.7% complete\n",
            "51.0% complete\n",
            "51.3% complete\n",
            "51.7% complete\n",
            "52.0% complete\n",
            "52.3% complete\n",
            "52.7% complete\n",
            "53.0% complete\n",
            "53.3% complete\n",
            "53.7% complete\n",
            "54.0% complete\n",
            "54.3% complete\n",
            "54.7% complete\n",
            "55.0% complete\n",
            "55.3% complete\n",
            "55.7% complete\n",
            "56.0% complete\n",
            "56.3% complete\n",
            "56.7% complete\n",
            "57.0% complete\n",
            "57.3% complete\n",
            "57.7% complete\n",
            "58.0% complete\n",
            "58.3% complete\n",
            "58.7% complete\n",
            "59.0% complete\n",
            "59.3% complete\n",
            "59.7% complete\n",
            "60.0% complete\n",
            "60.3% complete\n",
            "60.7% complete\n",
            "61.0% complete\n",
            "61.3% complete\n",
            "61.7% complete\n",
            "62.0% complete\n",
            "62.3% complete\n",
            "62.7% complete\n",
            "63.0% complete\n",
            "63.3% complete\n",
            "63.7% complete\n",
            "64.0% complete\n",
            "64.3% complete\n",
            "64.7% complete\n",
            "65.0% complete\n",
            "65.3% complete\n",
            "65.7% complete\n",
            "66.0% complete\n",
            "66.3% complete\n",
            "66.7% complete\n",
            "67.0% complete\n",
            "67.3% complete\n",
            "67.7% complete\n",
            "68.0% complete\n",
            "68.3% complete\n",
            "68.7% complete\n",
            "69.0% complete\n",
            "69.3% complete\n",
            "69.7% complete\n",
            "70.0% complete\n",
            "70.3% complete\n",
            "70.7% complete\n",
            "71.0% complete\n",
            "71.3% complete\n",
            "71.7% complete\n",
            "72.0% complete\n",
            "72.3% complete\n",
            "72.7% complete\n",
            "73.0% complete\n",
            "73.3% complete\n",
            "73.7% complete\n",
            "74.0% complete\n",
            "74.3% complete\n",
            "74.7% complete\n",
            "75.0% complete\n",
            "75.3% complete\n",
            "75.7% complete\n",
            "76.0% complete\n",
            "76.3% complete\n",
            "76.7% complete\n",
            "77.0% complete\n",
            "77.3% complete\n",
            "77.7% complete\n",
            "78.0% complete\n",
            "78.3% complete\n",
            "78.7% complete\n",
            "79.0% complete\n",
            "79.3% complete\n",
            "79.7% complete\n",
            "80.0% complete\n",
            "80.3% complete\n",
            "80.7% complete\n",
            "81.0% complete\n",
            "81.3% complete\n",
            "81.7% complete\n",
            "82.0% complete\n",
            "82.3% complete\n",
            "82.7% complete\n",
            "83.0% complete\n",
            "83.3% complete\n",
            "83.7% complete\n",
            "84.0% complete\n",
            "84.3% complete\n",
            "84.7% complete\n",
            "85.0% complete\n",
            "85.3% complete\n",
            "85.7% complete\n",
            "86.0% complete\n",
            "86.3% complete\n",
            "86.7% complete\n",
            "87.0% complete\n",
            "87.3% complete\n",
            "87.7% complete\n",
            "88.0% complete\n",
            "88.3% complete\n",
            "88.7% complete\n",
            "89.0% complete\n",
            "89.3% complete\n",
            "89.7% complete\n",
            "90.0% complete\n",
            "90.3% complete\n",
            "90.7% complete\n",
            "91.0% complete\n",
            "91.3% complete\n",
            "91.7% complete\n",
            "92.0% complete\n",
            "92.3% complete\n",
            "92.7% complete\n",
            "93.0% complete\n",
            "93.3% complete\n",
            "93.7% complete\n",
            "94.0% complete\n",
            "94.3% complete\n",
            "94.7% complete\n",
            "95.0% complete\n",
            "95.3% complete\n",
            "95.7% complete\n",
            "96.0% complete\n",
            "96.3% complete\n",
            "96.7% complete\n",
            "97.0% complete\n",
            "97.3% complete\n",
            "97.7% complete\n",
            "98.0% complete\n",
            "98.3% complete\n",
            "98.7% complete\n",
            "99.0% complete\n",
            "99.3% complete\n",
            "99.7% complete\n"
          ]
        }
      ],
      "source": [
        "#ODE Definition\n",
        "def ode(f, t, mu):\n",
        "    x, y = f\n",
        "    dfdt = [mu-x**2, -y]\n",
        "\n",
        "    return dfdt\n",
        "\n",
        "#Data generation\n",
        "DATA_POINTS = 50000 #Number of datapoints (pairs) to be collected for each selection of mu\n",
        "LOW_MU = -1.5 #Lower bound of the parameter to generate data\n",
        "HIGH_MU = 1.5 #Upper bound....\n",
        "MU_STEP_SIZE = 0.01 #How many steps in mu to take\n",
        "LOW_STATE_SPACE = -2 #Lower bound of the statespace sampling\n",
        "HIGH_STATE_SPACE = 2 #Upper bound...\n",
        "rng = np.random.default_rng(seed=42)\n",
        "\n",
        "#Linspace for time. Considers only one step of size DELTA\n",
        "DELTA = 0.03\n",
        "t=np.linspace(0,DELTA,2)\n",
        "#Numerically solve the ODE for a single step with random\n",
        "#initial conditions and parameter mu.\n",
        "mus= np.arange(LOW_MU, HIGH_MU, MU_STEP_SIZE)\n",
        "predictors = torch.zeros(size=(mus.size*DATA_POINTS, 3))\n",
        "responses = torch.zeros(size=(mus.size*DATA_POINTS, 2))\n",
        "for i in range(mus.size):\n",
        "    percentage = 100*i/mus.size\n",
        "    print(f'{percentage:.1f}% complete')\n",
        "    for j in range(DATA_POINTS):\n",
        "        #Sample the statespace uniformly (Is this effecient?)\n",
        "        z1 = rng.uniform(low=LOW_STATE_SPACE, high=HIGH_STATE_SPACE, size=(2))\n",
        "        z2 = odeint(ode, z1, t, args=(mus[i],))[1] #ode input gives answer at t0 and t1, so just select solution @ t1\n",
        "        predictors[i*DATA_POINTS+j] = torch.tensor((z1[0],z1[1],mus[i]))\n",
        "        responses[i*DATA_POINTS+j] = torch.tensor((z2[0],z2[1]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wZvy-6B1awQK"
      },
      "outputs": [],
      "source": [
        "# Create train/test split\n",
        "train_split = int(0.8 * len(predictors)) # 80% of data used for training set, 20% for testing\n",
        "pred_train, resp_train = predictors[:train_split], responses[:train_split]\n",
        "pred_test, resp_test = predictors[train_split:], responses[train_split:]\n",
        "\n",
        "pred_train = pred_train.to(device)\n",
        "resp_train = resp_train.to(device)\n",
        "pred_test = pred_test.to(device)\n",
        "resp_test = resp_test.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvtdrEJaemX6"
      },
      "source": [
        "Now we want to build our model. Based on the Su et al. 2021, we want to use a single modified resnet layer with 3 linear layers of 45 neurons each. (hidden layors, fully connected layors, whatever you want to call them). We have 2 state variables and 1 parameter, so the input of the whole neural network will have 3 neurons and the output will have 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WK9Nyagdfn6J"
      },
      "outputs": [],
      "source": [
        "NEURONS = 45  # Number of neurons in fully connected layers\n",
        "STATES = 2    # Number of state variables\n",
        "PARAMETERS = 1 # Number of parameters\n",
        "\n",
        "class suNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer_1 = nn.Linear(STATES + PARAMETERS, NEURONS)  # 3 inputs (state variables and parameter) and 45 outputs\n",
        "        self.layer_2 = nn.Linear(NEURONS, NEURONS)  # 45 inputs and 45 outputs\n",
        "        self.layer_3 = nn.Linear(NEURONS, STATES)  # 45 inputs and 2 outputs (the state variables are the two outputs)\n",
        "        self.layer_4 = nn.Linear(STATES,NEURONS)  # 45 inputs and 2 outputs (the state variables are the two outputs)\n",
        "        self.relu = nn.ReLU()\n",
        "    def forward(self, x):\n",
        "        input = x\n",
        "        x = self.layer_1(x) # Apply ReLU after the first layer\n",
        "        x = self.relu(x)\n",
        "        x = self.layer_2(x) # Apply/ ReLU after the second layer\n",
        "        x = self.relu(x)\n",
        "        x = self.layer_3(x)  # Third layer (no activation after this)\n",
        "        input2 = x + input[:, :STATES]  # Add the original state variables to the output\n",
        "        x = self.layer_4(input2) # Apply ReLU after the first layer\n",
        "        x = self.relu(x)\n",
        "        x = self.layer_2(x) # Apply/ ReLU after the second layer\n",
        "        x = self.relu(x)\n",
        "        x = self.layer_3(x)  # Third layer (no activation after this)\n",
        "        return x + input2[:, :STATES]  # Add the original state variables to the output\n",
        "\n",
        "model = suNet().to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoJcNJ2aoPSS"
      },
      "source": [
        "With our model defined we next need to define a loss function. The loss function is given in Su's paper as\n",
        "$$L(\\Theta) =\\frac{1}{J} \\sum_{j=1}^J \\| z_j^{(2)} - N(z_j^{(1)},\\alpha,\\Theta)\\|^2_2 $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "L6TCMaHgkwaJ"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooyKnHR_w1Tr",
        "outputId": "5da22080-fc39-4708-c27c-4e87facef6cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "Precentage: 0.0 | Epoch: 0 | Loss: 0.11712608| Test loss: 0.10198291\n",
            "Precentage: 1.0 | Epoch: 100 | Loss: 0.00022734| Test loss: 0.00083276\n",
            "Precentage: 2.0 | Epoch: 200 | Loss: 0.00010397| Test loss: 0.00034681\n",
            "Precentage: 3.0 | Epoch: 300 | Loss: 0.00006547| Test loss: 0.00020319\n"
          ]
        }
      ],
      "source": [
        "print(pred_train.is_cuda)\n",
        "# Calculate accuracy (a classification metric)\n",
        "def accuracy_fn(y_true, y_pred):\n",
        "    correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal\n",
        "    acc = (correct / len(y_pred)) * 100\n",
        "    return acc\n",
        "\n",
        "epochs = 10000\n",
        "for epoch in range(epochs):\n",
        "    ### Training\n",
        "    model.train()\n",
        "\n",
        "    # 1. Forward pass (model outputs raw logits)\n",
        "    resp_pred= model(pred_train).squeeze() # squeeze to remove extra `1` dimensions, this won't work unless model and data are on same device\n",
        "\n",
        "    # 2. Calculate loss/accuracy\n",
        "    loss = loss_fn(resp_pred, resp_train)\n",
        "    #acc = accuracy_fn(y_true=resp_train, y_pred=resp_pred)\n",
        "\n",
        "    # 3. Optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # 4. Loss backwards\n",
        "    loss.backward()\n",
        "\n",
        "    # 5. Optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "    ### Testing\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        # 1. Forward pass\n",
        "        test_logits = model(pred_test).squeeze()\n",
        "        # 2. Caculate loss/accuracy\n",
        "        test_loss = loss_fn(test_logits,\n",
        "                            resp_test)\n",
        "\n",
        "    # Print out what's happening every 10 epochs\n",
        "    if epoch % 100 == 0:\n",
        "        percentage = 100*epoch/epochs\n",
        "        print(f\"Precentage: {percentage:.1f} | Epoch: {epoch} | Loss: {loss:.8f}| Test loss: {test_loss:.8f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbsH98hbNW4F"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "INIT_X=0.2\n",
        "INIT_Y=0.2\n",
        "MU = 1\n",
        "STEPS = 20000\n",
        "TIME_LENGTH = STEPS * DELTA\n",
        "RESOLUTION = 2000 #Number of time steps in the ODE simulation\n",
        "#initial conditions to test our model from\n",
        "init = np.array([INIT_X,INIT_Y,MU])\n",
        "#Initialized the predictions\n",
        "model_prediction_x = np.zeros(STEPS)\n",
        "model_prediction_y = np.zeros(STEPS)\n",
        "#Convert the init to a torch to input into the model (DO NOT TOUCH)\n",
        "input = torch.transpose(torch.from_numpy(init).unsqueeze(1).float(),0,1)\n",
        "input = input.to(device)\n",
        "with torch.inference_mode():\n",
        "    for i in range(STEPS):\n",
        "        output = model(input)\n",
        "        #print(output)\n",
        "        model_prediction_x[i]= output[0][0].item()\n",
        "        model_prediction_y[i]= output[0][1].item()\n",
        "        #Add the parameter back onto the output\n",
        "        c=torch.tensor([[MU]]).to(device)\n",
        "\n",
        "        #The new input is the old output\n",
        "        input = torch.cat((output,c),dim=1)\n",
        "\n",
        "        '''if (i% 10 == 0):\n",
        "            print(f'State position: {output}| time t={time[i]}')'''\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xC4pl8pZOBdY"
      },
      "outputs": [],
      "source": [
        "#Plot the model predicitons\n",
        "s = np.linspace(0, TIME_LENGTH, STEPS)\n",
        "#Plot the true solutions\n",
        "time = np.linspace(0,TIME_LENGTH,RESOLUTION) #The step size should be aligned with time step in the model\n",
        "true = odeint(ode, [INIT_X,INIT_Y], time, args=(MU,))\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "#Time Series\n",
        "ax1.plot(time, true[:,0])\n",
        "ax1.plot(time, true[:,1])\n",
        "ax1.scatter(s, model_prediction_x, s=4, c='blue', edgecolor='w', alpha=0.15, linewidth=0.5, marker='o')\n",
        "ax1.scatter(s, model_prediction_y, s=4, c='#FF5733', edgecolor='w', alpha=0.15, linewidth=0.5, marker='o')\n",
        "\n",
        "# Add titles and labels\n",
        "ax1.set_title('Time Series')\n",
        "ax1.legend([\"Model x(t_i) prediction\", \"Model y(t_i) prediction\", \"True x(t)\", \"True y(t)\"])\n",
        "ax1.set_xlabel('t')\n",
        "ax1.set_ylabel('x or y')\n",
        "#Phaseplane\n",
        "ax2.plot(true[:,0],true[:,1])\n",
        "ax2.scatter(model_prediction_x,model_prediction_y, s=4, c='red', alpha=0.15, linewidth=0.5, marker='o')\n",
        "ax2.set_title('Phase Plane')\n",
        "ax2.set_xlabel('X Axis')\n",
        "ax2.set_ylabel('Y Axis')\n",
        "print(model_prediction_x)\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}